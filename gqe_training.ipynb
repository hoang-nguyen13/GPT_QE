{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generative quantum eigensolver (GQE) training using PennyLane data\n===================================================\n\nWe will be demonstrating and evaluating a novel algorithm proposed by\nNakaji et al. in their paper [The generative quantum eigensolver (GQE)\nand its application for ground state\nsearch](https://arxiv.org/abs/2401.09253) that employs a classical\ngenerative model of quantum circuits for the purpose of ground-state\nenergy estimation of any molecular Hamiltonian. It has been proposed as\na scalable alternative to the [variational quantum eigensolver\n(VQE)](https://pennylane.ai/qml/demos/tutorial_vqe/) approach, where the\nquantum state is represented as a quantum circuit with tunable\nparameters which are then optimized during training in order to arrive\nat a state minimizing the corresponding energy $E.$ Instead, in GQE, the\nstructure of the quantum circuit is given by a trained generative model.\n\nWe will primarily focus on offline training on a fixed training dataset\n\\-- thanks to the molecular data available in [PennyLane\nDatasets](https://pennylane.ai/datasets/). By the end of the demo, we\nwill show that the model gradually provides a better estimate for the\nenergies and, in turn, can sample energies close to the ground state\nenergy calculated by PennyLane.\n\nThis demo is organized as follows. Firstly, we compare the GQE and VQE\nalgorithms, and afterwards, we dive deeper in describing GPT-QE, i.e.,\nthe GQE algorithm which uses a GPT model, and its training. Next, we\ngenerate the training dataset for our GPT model using PennyLane, and\ngive details on our model architecture and training implementation.\nAfter that, we evaluate the model throughout its training and discuss\nits performance in estimating the ground state. And lastly, we discuss\nthe results, potential ways optimizing the code, and its extension into\nthe \\\"online training\\\" phase.\n\n![](../_static/demo_thumbnails/opengraph_demo_thumbnails/OGthumbnail_generative_quantum_eigensolver.png){.align-centeravailable\nwidth=\"70.0%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GQE vs. VQE\n\nDespite the relative success of VQE, there are some issues regarding its\n*trainability* for large problem instances. This shortcoming makes it\nless competitive against the performance of classical machine learning\n(ML) algorithms for large problems. To bypass this, the GQE algorithm\nwas proposed. Specifically, GQE uses a classical generative model where\nquantum circuits are sampled as a sequence of unitaries from a given\noperator pool. This generative model is then trained so that it learns\nto predict quantum circuits that evolves an initial state to the states\nbetter approximating the ground state.\n\nThe main difference between the two approaches is where the tunable\nparameters are embedded. That is, it is the classical GQE model that is\nbeing optimized as opposed to the variable quantum circuit of VQE.\nPotentially then, the\n`barren plateau </demos/tutorial_barren_plateaus/>`{.interpreted-text\nrole=\"doc\"} landscape of VQE and the quantum gradient evaluation of\nlarge circuits will be sidestepped by GQE, thus becoming more amenable\nfor larger problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<figure>\n<img\nsrc=\"../_static/demonstration_assets/gqe_training/paper_gqe_vqe.png\"\nclass=\"align-center\" style=\"width:90.0%\"\nalt=\"../_static/demonstration_assets/gqe_training/paper_gqe_vqe.png\" />\n<figcaption>Figure 1: Diagrams of GQE and VQE from Fig. 1\nin</figcaption>\n</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-QE background\n\nIn particular, the model architecture used by Nakaji et al. for GQE was\na generative pre-trained transformer (GPT),. This choice is then\nreflected in the name GPT-QE. As language models, GPTs are successful in\ngenerating sequences of words that closely resemble human natural\nlanguage. This performance is harnessed for quantum chemistry by\nconstructing quantum states $\\rho$ as a sequence of unitary operators\nwhich are, in turn, represented by quantum circuits. That is, we let\n$\\rho = U\\rho_0 U^{\\dagger}$ for some fixed initial state $\\rho_0$ and\nthe aforementioned sequence is $U = U_{j_N}U_{j_{N-1}}\\cdots U_{j_1}.$\nThe GPT model samples a sequence of integers $j_1, j_2, ..., j_N$\nindexing a pool of operators $U_j$ generated using molecular data from\n[PennyLane Molecules](https://pennylane.ai/datasets/qchem). We interpret\nthese integers as tokens and the pool as the vocabulary in the parlance\nfor language models. The goal of training is then to minimize the\ncorresponding energy $E = \\mbox{Tr}(\\hat{H}\\rho),$ where $\\hat{H}$ is\nthe Hamiltonian of the molecule in question.\n\nEach token $j_i$ is sampled from the distribution\n$\\exp(-\\beta w_{j_i}),$ where $w_{j_i}$ is the unnormalized log\nprobability (or logit) returned by the GPT model for the token $j_i$ and\n$\\beta$ is an inverse temperature representing a trade-off parameter\nbetween exploration and exploitation. We then observe that the\nprobability of sampling a state through the method described above is\nproportional to $\\exp(-\\beta w_{\\mbox{sum}}),$ where\n$w_{\\mbox{sum}} = \\sum_{i=1}^N w_{j_i}$ and the probability for the\ncorresponding energy is $\\exp(-\\beta E).$ We thus have a constraint for\nthe total logit to be equal to the energy of the corresponding state:\n$w_{\\mbox{sum}} = E,$ which can be imposed by training GPT-QE to\nminimize the loss function $C = (w_{\\mbox{sum}} - E)^2.$ With this\nconstraint satisfied, GPT-QE would then be sampling states of smaller\nenergies with increasing likelihood.\n\nMore concretely, we summarize the *pre-*training loop in Figure 2. This\nis called pre-training because it involves learning from a fixed dataset\nfirst before transitioning to the \\\"real\\\" training which utilizes the\ndata generated by the model itself. In this demo, we will call the\npre-training as offline training since the GPT model does not receive\nfeedback from the sequences it samples, and online training if\notherwise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<figure>\n<img\nsrc=\"../_static/demonstration_assets/gqe_training/gqe_training_diagram.png\"\nclass=\"align-center\" style=\"width:90.0%\"\nalt=\"../_static/demonstration_assets/gqe_training/gqe_training_diagram.png\" />\n<figcaption>Figure 2: Overview for offline training of\nGPT-QE</figcaption>\n</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset construction via PennyLane\n\nFirstly, let us construct the static dataset we will use for offline\ntraining. We choose to generate our own dataset in order to illustrate\nthe sequences and energies more concretely. Our dataset will be made\nfrom random sequences of tokens, which we recall corresponds to indices\nof a vocabulary of unitary operators. We then define an energy function\nin PennyLane to calculate the energy of a state corresponding to a token\nsequence. Applying the aforementioned function, we would then have a\ndataset of token sequences and energies for the GPT model offline\ntraining.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading molecular information\n\nFor simplicity, let us consider the [hydrogen\nmolecule](https://pennylane.ai/datasets/qchem/h2-molecule) and load the\ncorresponding dataset from PennyLane. Recall that we would need a\nvocabulary of operators $U_j$, an initial state $\\rho_0,$ and the\nHamiltonian $\\hat{H}$ for a hydrogen molecule. We also get the ground\nstate energy for later comparison with the results.\n\nSpecifically, the unitary operators $U_j$ are time evolution operators\nas prescribed in. The non-identity operators are generated in PennyLane\nusing `~.pennylane.SingleExcitation`{.interpreted-text role=\"class\"} and\n`~.pennylane.DoubleExcitation`{.interpreted-text role=\"class\"} which\nthen depend on the number of electrons and orbitals of the molecule.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pennylane as qml\n\ndef generate_molecule_data(molecules=\"H2\"):\n    datasets = qml.data.load(\"qchem\", molname=molecules)\n\n    # Get the time set T\n    op_times = np.sort(np.array([-2**k for k in range(1, 5)] + [2**k for k in range(1, 5)]) / 160)\n\n    # Build operator set P for each molecule\n    molecule_data = dict()\n    for dataset in datasets:\n        molecule = dataset.molecule\n        num_electrons, num_qubits = molecule.n_electrons, 2 * molecule.n_orbitals\n        singles, doubles = qml.qchem.excitations(num_electrons, num_qubits)\n        double_excs = [qml.DoubleExcitation(time, wires=double) for double in doubles for time in op_times]\n        single_excs = [qml.SingleExcitation(time, wires=single) for single in singles for time in op_times]\n        identity_ops = [qml.exp(qml.I(range(num_qubits)), 1j*time) for time in op_times] # For Identity\n        operator_pool = double_excs + single_excs + identity_ops\n        molecule_data[dataset.molname] = {\n            \"op_pool\": np.array(operator_pool), \n            \"num_qubits\": num_qubits,\n            \"hf_state\": dataset.hf_state,\n            \"hamiltonian\": dataset.hamiltonian,\n            \"expected_ground_state_E\": dataset.fci_energy\n        }\n    return molecule_data\n\nmolecule_data = generate_molecule_data(\"H2\")\nh2_data = molecule_data[\"H2\"]\nop_pool = h2_data[\"op_pool\"]\nnum_qubits = h2_data[\"num_qubits\"]\ninit_state = h2_data[\"hf_state\"]\nhamiltonian = h2_data[\"hamiltonian\"]\ngrd_E = h2_data[\"expected_ground_state_E\"]\nop_pool_size = len(op_pool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Defining the energy function\n\nIn PennyLane, we define the energy function\n$E = \\mbox{Tr}(\\hat{H}U_{j_N}\\cdots U_{j_1}\\rho_0 U_{j_1}^{\\dagger}\\cdots U_{j_N}^{\\dagger})$\ncorresponding to Eq. 1 of. Here, `energy_circuit` takes in the operator\nsequence $U_{j_1}, U_{j_2}, ..., U_{j_N}$ and returns the energy of the\ncorresponding quantum state.\n\nAs a slight extension, we can also calculate the energies for each\nsubsequence of operators to help with the training of the model. That\nis, for a sequence of three operators $U_{j_1}, U_{j_2}, U_{j_3}$ we\ncompute the energies for $U_{j_1}$ and $U_{j_1}, U_{j_2}$ instead of\njust the full sequence of three operators, which was described in. This\ncan be done simply in PennyLane, using\n`~.pennylane.Snapshot`{.interpreted-text role=\"class\"} as shown below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=num_qubits)\n\n@qml.qnode(dev)\ndef energy_circuit(gqe_ops):\n    # Computes Eq. 1 from Nakaji et al. based on the selected unitary operators\n    qml.BasisState(init_state, wires=range(num_qubits)) # Initial state <-- Hartree Fock state\n    for op in gqe_ops:\n        qml.Snapshot(measurement=qml.expval(hamiltonian))\n        qml.apply(op) # Applies each of the unitary operators\n    return qml.expval(hamiltonian)\n\nenergy_circuit = qml.snapshots(energy_circuit)\n\ndef get_subsequence_energies(op_seq):\n    # Collates the energies of each subsequence for a batch of sequences\n    energies = []\n    for ops in op_seq:\n        es = energy_circuit(ops)\n        energies.append(\n            [es[k].item() for k in list(range(1, len(ops))) + [\"execution_results\"]]\n        )\n    return np.array(energies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Token sequence generation with corresponding energies\n\nWith these ingredients, we can now construct a dataset containing\nsequences of tokens and their energies. Since we cannot feed the\noperators directly to the GPT model, we would need to tokenize them. The\nindices of `op_pool` seems to be a good candidate, but we instead choose\nthe tokens to be the `op_pool` indices shifted by 1. This is so that we\ncan define a special token `0` that tells the GPT model where the\nsequence starts.\n\nWe generate a `train_size` number of random operator sequences of length\n`seq_len` for our purposes and calculate their energies (and their\nsubsequences).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate sequence of indices of operators in vocab\ntrain_size = 1024\nseq_len = 4\ntrain_op_pool_inds = np.random.randint(op_pool_size, size=(train_size, seq_len))\n\n# Corresponding sequence of operators\ntrain_op_seq = op_pool[train_op_pool_inds]\n\n# Corresponding tokens with special starting tokens\ntrain_token_seq = np.concatenate([\n    np.zeros(shape=(train_size, 1), dtype=int), # starting token is 0\n    train_op_pool_inds + 1 # shift operator inds by one\n], axis=1)\n\n# Calculate the energies for each subsequence in the training set\ntrain_sub_seq_en = get_subsequence_energies(train_op_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-QE offline training\n\nHaving setup our training dataset, we can start implementing our offline\ntraining loop as illustrated in Figure 2. We outline our implementation\nbelow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT model implementation details\n\nThe GPT model we will use in this demo is mostly implemented in the\n[nanoGPT repo](https://github.com/karpathy/nanoGPT) (a reimplementation\nof the [OpenAI GPT-2](https://github.com/openai/gpt-2)) as the\n[class](https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/model.py#L118)\n`GPT` with the model hyperparameters stored in the\n[dataclass](https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/model.py#L109)\n`GPTConfig`. Namely, we will use 12 attention layers, 12 attention\nheads, and 768 embedding dimensions, which are equal to those described\nin. We can import from the nanoGPT repo directly by running the `curl`\ncommand (commented out below) in a Jupyter Notebook. Since nanoGPT is\ntrained as a language model, its loss function and sampling method are\ndefined differently. We then define the subclass `GPTQE` below to\noverride some nanoGPT methods in order to make it more suitable for our\ncase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# !curl -O https://raw.githubusercontent.com/karpathy/nanoGPT/master/model.py\nfrom model import GPT, GPTConfig\nimport torch\nfrom torch.nn import functional as F\n\nclass GPTQE(GPT):\n    def forward(self, idx):\n        device = idx.device\n        b, t = idx.size()\n        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x)\n        return logits\n    \n    def calculate_loss(self, tokens, energies):\n        current_tokens, next_tokens = tokens[:, :-1], tokens[:, 1:]\n        # calculate the logits for the next possible tokens in the sequence\n        logits = self(current_tokens)\n        # get the logit for the actual next token in the sequence\n        next_token_mask = torch.nn.functional.one_hot(\n            next_tokens, num_classes=self.config.vocab_size\n        )\n        next_token_logits = (logits * next_token_mask).sum(axis=2)\n        # calculate the cumulative logits for each subsequence\n        cumsum_logits = torch.cumsum(next_token_logits, dim=1)\n        # match cumulative logits to subsequence energies\n        loss = torch.mean(torch.square(cumsum_logits - energies))\n        return loss\n    \n    @torch.no_grad()\n    def generate(self, n_sequences, max_new_tokens, temperature=1.0, device=\"cpu\"):\n        idx = torch.zeros(size=(n_sequences, 1), dtype=int, device=device)\n        total_logits = torch.zeros(size=(n_sequences, 1), device=device)\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            # forward the model to get the logits for the index in the sequence\n            logits = self(idx_cond)\n            # pluck the logits at the final step \n            logits = logits[:, -1, :] \n            # set the logit of the first token so that its probability will be zero\n            logits[:, 0] = float(\"inf\")\n            # apply softmax to convert logits to (normalized) probabilities and scale by desired temperature\n            probs = F.softmax(-logits / temperature, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # # Accumulate logits\n            total_logits += torch.gather(logits, index=idx_next, dim=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx, total_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, it is important to note that the loss function\n`calculate_loss`, that we defined is different from the one described in\nwhich is $(\\exp(-w_{\\mbox{sum}}) - \\exp(-E))^2.$ As described\nbeforehand, we instead directly compute the mean squared error between\n$w_{\\mbox{sum}}$ and $E.$ Since the exponential function is one-to-one,\nboth loss functions would then impose the same minimum. Using the error\nbetween exponentials may even introduce numerical instabilities in the\ntraining since the loss would be taking differences of potentially large\nnumbers. In addition to this change from, we also use the error between\nthe cumulative sum of logits and the corresponding energy for each\nsubsequence instead of just the error between total logits and the\nenergy of an entire sequence. This addition will give more training data\nto the model and should help with logit matching the intermediate\ntokens.\n\nSince it was not explicitly shown in, another possible deviation we made\nis with the logit calculation during offline training. It seems that the\nlogits were accumulated by looping through the sequential generation of\ntokens. In order to fill in the blanks, we implement the logit\ncalculation in a manner that we think is efficient. Namely, we directly\npass the fixed training sequences to the model and retrieve the relevant\nlogits from it. This can be done because we are using a causal mask for\nthe attention blocks so that the logits of the earlier tokens in the\nsequence are not affected by tokens in the later part of the sequence.\nThus, having the same effect of the sequential token generation.\n\nWe initialize our GPT model below and we see that it has around 85\nmillion parameters. When saved, the model is `324.25 MB` in size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tokens = torch.from_numpy(train_token_seq).to(\"cuda\")\nenergies = torch.from_numpy(train_sub_seq_en).to(\"cuda\")\n\ngpt = GPTQE(GPTConfig(\n    vocab_size=op_pool_size + 1,\n    block_size=seq_len,\n    dropout=0.2,\n    bias=False\n)).to(\"cuda\")\nopt = gpt.configure_optimizers(\n    weight_decay=0.01, learning_rate=5e-5, betas=(0.9, 0.999), device_type=\"cuda\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: rst-class\nsphx-glr-script-out\n\n``` none\nnumber of parameters: 84.98M\nnum decayed parameter tensors: 50, with 84,963,072 parameters\nnum non-decayed parameter tensors: 25, with 19,200 parameters\n```\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT offline training\n\nWe now implement a training loop for our GPT model. This can be framed\nas a straightforward supervised learning problem. We sketch the steps\nfor each training iteration/epoch below:\n\n1.  Shuffle the training set and split it into `n_batches` minibatches\n2.  For each minibatch, calculate the average loss, the gradients, and\n    take an optimizer step\n3.  For each n-th iteration (n is 500 here), evaluate the GPT model:\n    -   Generate a batch of sequences and the predicted energies (total\n        logits). Note that these are not necessarily same sequences as\n        in the training set.\n    -   Calculate the true energies using PennyLane\n    -   Calculate the mean absolute error as a metric to track the\n        learning progress and save the GPT model everytime the metric\n        gets better\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_batches = 8\ntrain_inds = np.arange(train_size)\n\nlosses = []\npred_Es_t = []\ntrue_Es_t = []\ncurrent_mae = 10000\ngpt.train()\nfor i in range(10000):\n    # Shuffle batches of the training set\n    np.random.shuffle(train_inds)\n    token_batches = torch.tensor_split(tokens[train_inds], n_batches)\n    energy_batches = torch.tensor_split(energies[train_inds], n_batches)\n    \n    # SGD on random minibatches\n    loss_record = 0\n    for token_batch, energy_batch in zip(token_batches, energy_batches):\n        opt.zero_grad()\n        loss = gpt.calculate_loss(token_batch, energy_batch)\n        loss.backward()\n        opt.step()\n        loss_record += loss.item() / n_batches\n    losses.append(loss_record)\n\n    if (i+1) % 500 == 0:\n        # For GPT evaluation\n        gpt.eval()\n        gen_token_seq, pred_Es = gpt.generate(\n            n_sequences=100, \n            max_new_tokens=seq_len, \n            temperature=0.001, # Use a low temperature to emphasize the difference in logits\n            device=\"cuda\"\n        )\n        pred_Es = pred_Es.cpu().numpy()\n\n        gen_inds = (gen_token_seq[:, 1:] - 1).cpu().numpy()\n        gen_op_seq = op_pool[gen_inds]\n        true_Es = get_subsequence_energies(gen_op_seq)[:, -1].reshape(-1, 1)\n\n        mae = np.mean(np.abs(pred_Es - true_Es))\n        ave_E = np.mean(true_Es)\n        \n        pred_Es_t.append(pred_Es)\n        true_Es_t.append(true_Es)\n        \n        print(f\"Iteration: {i+1}, Loss: {losses[-1]}, MAE: {mae}, Ave E: {ave_E}\")\n        \n        if mae < current_mae:\n            current_mae = mae\n            torch.save(gpt, f\"./seq_len={seq_len}/gqe.pt\")\n            print(\"Saved model!\")\n            \n        gpt.train()\n        \npred_Es_t = np.concatenate(pred_Es_t, axis=1)\ntrue_Es_t = np.concatenate(true_Es_t, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: rst-class\nsphx-glr-script-out\n\n``` none\nIteration: 500, Loss: 0.004496691238049528, MAE: 0.13945468622863236, Ave E: -1.1161227981406456\nSaved model!\nIteration: 1000, Loss: 0.001162520404255374, MAE: 0.11792013497926974, Ave E: -1.116178063434579\nSaved model!\nIteration: 1500, Loss: 0.0006311882560414964, MAE: 0.08421050347067748, Ave E: -1.1304435666682537\nSaved model!\nIteration: 2000, Loss: 0.0002220232025956396, MAE: 0.03313205549288038, Ave E: -1.13411711385679\nSaved model!\nIteration: 2500, Loss: 9.021296506465553e-05, MAE: 0.03720317687198404, Ave E: -1.1360217383940532\nIteration: 3000, Loss: 0.00011929328764308375, MAE: 0.010246824522607662, Ave E: -1.1355033629645301\nSaved model!\nIteration: 3500, Loss: 4.015137835017087e-05, MAE: 0.008332604993116905, Ave E: -1.1362737218253494\nSaved model!\nIteration: 4000, Loss: 0.00025425587370956726, MAE: 0.03346923599957368, Ave E: -1.13442109812976\nIteration: 4500, Loss: 4.590269966149363e-05, MAE: 0.0086580669691949, Ave E: -1.1344678899103924\nIteration: 5000, Loss: 2.7407370499136962e-05, MAE: 0.006680762382889203, Ave E: -1.136412143925528\nSaved model!\nIteration: 5500, Loss: 3.778071550021417e-05, MAE: 0.014272903220676704, Ave E: -1.1362969016861684\nIteration: 6000, Loss: 2.2792776141250974e-05, MAE: 0.007428675818214263, Ave E: -1.1367647064449693\nIteration: 6500, Loss: 1.9002385742602413e-05, MAE: 0.004431537870071902, Ave E: -1.135880723613281\nSaved model!\nIteration: 7000, Loss: 1.5268728079291623e-05, MAE: 0.002464256235883442, Ave E: -1.1356989137037925\nSaved model!\nIteration: 7500, Loss: 1.1030378864566936e-05, MAE: 0.007000517223791054, Ave E: -1.1360445255294285\nIteration: 8000, Loss: 7.638036884241474e-06, MAE: 0.0044611951680048586, Ave E: -1.1352658877947734\nIteration: 8500, Loss: 1.616690860258467e-05, MAE: 0.004094392133172753, Ave E: -1.1356437076129735\nIteration: 9000, Loss: 7.37882245331426e-06, MAE: 0.004240113290004896, Ave E: -1.1358971131175264\nIteration: 9500, Loss: 1.004411104422562e-05, MAE: 0.010631562300185794, Ave E: -1.1368761600775912\nIteration: 10000, Loss: 1.809862392776087e-05, MAE: 0.01987725166307399, Ave E: -1.1345492765523346\nCPU times: user 2h 12min 24s, sys: 8.18 s, total: 2h 12min 32s\nWall time: 2h 12min 32s\n```\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With a preliminary look at the training logs above, we see that the\noffline training took 2 h and 12 min for 10,000 training iterations. The\ncode execution time was measured by including the `%%time` magic command\nbefore the code block. We also note that the mean absolute error between\nthe predicted and true energies for the generated sequences quickly\nbecame smaller during the earlier parts of the training but fluctuates\nmore later on. As mentioned earlier, a model version is saved each time\nwe get better performance. The best model version is then saved at the\n7,000th iteration, where the mean absolute error is at its lowest. The\nother quantities we observe are the average true energies of the\ngenerated sequences. It is then promising to see that throughout\ntraining, this is close to the ground state energy\n`grd_E=-1.1372633205048763 Ha`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-QE results\n\nHaving finished the offline training, let\\'s take a look at some of our\nresults.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training loss curve\n\nOne of the first things we can look at is the training loss curve.\nRecall that for our case, the loss is the mean squared error between the\ncumulative sum of logits (predicted subsequence energies) and their\ncorresponding true subsequence energies. So reducing this quantity\nallows the model to be better at giving correct energies and in turn,\ncorrectly sample sequences with lower energies. Since the raw loss\nvalues become very small, we instead plot the loss in log-scale below to\nmagnify the loss trend.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import holoviews as hv\nimport hvplot.pandas\nimport pandas as pd\n\nhvplot.extension('matplotlib')\n\nlosses = pd.read_csv(\"./seq_len=4/trial7/losses.csv\")[\"0\"]\nloss_fig = losses.hvplot(\n    title=\"Training loss progress\", ylabel=\"loss\", xlabel=\"Training epochs\", logy=True\n).opts(fig_size=600, fontscale=2, aspect=1.2)\nloss_fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<figure>\n<img\nsrc=\"../_static/demonstration_assets/gqe_training/gqe_training_loss.png\"\nclass=\"align-center\" style=\"width:100.0%\"\nalt=\"../_static/demonstration_assets/gqe_training/gqe_training_loss.png\" />\n<figcaption>Figure 3: The subsequence loss for each training\niteration</figcaption>\n</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see in Figure 3 that the loss continues to decrease until around the\n4,000th iteration. There, the model was erroraneous but is quick to\nrecover as training continues. This may signal that the GPT model\nstarted focusing on learning something erroraneous too quickly. So, more\nregularization noise (like `dropout`) may be needed to help avoid this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation progress\n\nWe now track the performance of the GPT model throughout its training in\nFigure 4 below. As mentioned before, after every 500th iteration, we let\nthe model generate a batch of sequences. Alongside, we also return the\ntotal logits (predicted energies) used in the sequence generation. In\nFigure 4, the average predicted energies correspond to the red markers\nand the distribution of predicted energies is represented by the red\narea. Once we have the generated sequences, we can also let PennyLane\ncalculate the true sequence energies. Similarly then, the blue markers\nare the average true energies and the blue area represents the true\nenergy distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_true = pd.read_csv(\"./seq_len=4/trial7/true_Es_t.csv\").iloc[:, 1:]\ndf_pred = pd.read_csv(\"./seq_len=4/trial7/pred_Es_t.csv\").iloc[:, 1:]\n\ndf_true.columns = df_true.columns.astype(int)\ndf_pred.columns = df_pred.columns.astype(int)\n\ndf_trues_stats = pd.concat([df_true.mean(axis=0), df_true.min(axis=0), df_true.max(axis=0)], axis=1).reset_index()\ndf_trues_stats.columns = [\"Training Iterations\", \"Ave True E\", \"Min True E\", \"Max True E\"]\n\ndf_preds_stats = pd.concat([df_pred.mean(axis=0), df_pred.min(axis=0), df_pred.max(axis=0)], axis=1).reset_index()\ndf_preds_stats.columns = [\"Training Iterations\", \"Ave Pred E\", \"Min Pred E\", \"Max Pred E\"]\n\nfig = (\n    df_trues_stats.hvplot.scatter(x=\"Training Iterations\", y=\"Ave True E\", label=\"Mean True Energies\") * \n    df_trues_stats.hvplot.line(x=\"Training Iterations\", y=\"Ave True E\", alpha=0.5, linewidth=1) * \n    df_trues_stats.hvplot.area(x=\"Training Iterations\", y=\"Min True E\", y2=\"Max True E\", alpha=0.1)\n) * (\n    df_preds_stats.hvplot.scatter(x=\"Training Iterations\", y=\"Ave Pred E\", label=\"Mean Predicted Energies\") * \n    df_preds_stats.hvplot.line(x=\"Training Iterations\", y=\"Ave Pred E\", alpha=0.5, linewidth=1) * \n    df_preds_stats.hvplot.area(x=\"Training Iterations\", y=\"Min Pred E\", y2=\"Max Pred E\", alpha=0.1)\n)\nfig = fig * hv.Curve([[0, grd_E], [10000, grd_E]], label=\"Ground State Energy\").opts(color=\"k\", alpha=0.4, linestyle=\"dashed\")\nfig = fig.opts(ylabel=\"Sequence Energies\", title=\"GQE Evaluations\", fig_size=600, fontscale=2)\nfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<figure>\n<img\nsrc=\"../_static/demonstration_assets/gqe_training/gqe_performance.png\"\nclass=\"align-center\" style=\"width:100.0%\"\nalt=\"../_static/demonstration_assets/gqe_training/gqe_performance.png\" />\n<figcaption>Figure 4: True and predicted energies for sequences\ngenerated by the GPT model for each 500th training\niteration</figcaption>\n</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now see that the energies predicted by the model improve in accuracy,\naligning more closely with the true energies during training. The\nincrease in accuracy then allows the model to correctly sample states\nwith lower energies. This is supported in Figure 4 where the sampled\ntrue energies get closer to the ground state energy (the dashed line).\n\nNote that at around the 4,000th iteration, the predicted energies are\nvery far from the true energies. This makes sense considering our\nobservation in Figure 3. Also note that at the 7,000th iteration, the\naverages of the predicted and true energies are the closest and even\ntheir respective spreads seem to have good overlap. This is when the\nbest performing model version was saved, as seen in the training logs.\nFor later iterations however, the predicted energies no longer improved.\nThis may indicate that the GPT model has started overfitting on the\ntraining dataset in the later iterations. That is, the model became\ngreat at predicting the correct energies for the training set (as\nobserved by the decreasing loss in Figure 3) but not great at\ngeneralizing on those outside the training set (like the sequences that\nthe model generated on its own). One solution to avoid overfitting could\nthen be online training. This is so that the GPT model is not restricted\non a fixed dataset on which to overfit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sequence generation comparison\n\nHere, we compare some statistics of the true energies corresponding to\nsequences generated by a \\\"random\\\" model, the latest version of the\nmodel after all the training iterations, and the best model version\nsaved based on the mean absolute error between the true and predicted\nenergies during training (for our case, this is the model saved at the\n7,000th iteration). Note that we consider the training set to be\ngenerated by a random model since the token sequences are just sampled\nuniformly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Latest model\ngen_token_seq_, _ = gpt.generate(\n    n_sequences=1024, \n    max_new_tokens=seq_len, \n    temperature=0.001, \n    device=\"cuda\"\n)\ngen_inds_ = (gen_token_seq_[:, 1:] - 1).cpu().numpy()\ngen_op_seq_ = op_pool[gen_inds_]\ntrue_Es_ = get_subsequence_energies(gen_op_seq_)[:, -1].reshape(-1, 1)\n\n# Best model\nloaded = torch.load(\"./seq_len=4/trial7/gqe.pt\")\nloaded_token_seq_, _ = loaded.generate(\n    n_sequences=1024, \n    max_new_tokens=seq_len, \n    temperature=0.001, \n    device=\"cuda\"\n)\nloaded_inds_ = (loaded_token_seq_[:, 1:] - 1).cpu().numpy()\nloaded_op_seq_ = op_pool[loaded_inds_]\nloaded_true_Es_ = get_subsequence_energies(loaded_op_seq_)[:, -1].reshape(-1, 1)\n\n# Summary table\ndf_compare_Es = pd.DataFrame({\n    \"Source\": [\"Random\", \"Latest Model\", \"Best Model\"], \n    \"Aves\": [train_sub_seq_en[:, -1].mean(), true_Es_.mean(), loaded_true_Es_.mean()],\n    \"Mins\": [train_sub_seq_en[:, -1].min(), true_Es_.min(), loaded_true_Es_.min()],\n    \"Maxs\": [train_sub_seq_en[:, -1].max(), true_Es_.max(), loaded_true_Es_.max()],\n    \"Mins_error\": [\n        abs(train_sub_seq_en[:, -1].min() - grd_E),\n        abs(true_Es_.min() - grd_E),\n        abs(loaded_true_Es_.min() - grd_E),\n    ],\n})\ndf_compare_Es"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: rst-class\nsphx-glr-script-out\n\n``` none\nSource      Aves      Mins      Maxs   Mins_error\n0        Random -1.114531 -1.136982 -1.027878 2.811117e-04\n1  Latest Model -1.132200 -1.137038 -1.125118 2.253048e-04\n2    Best Model -1.135560 -1.137263 -1.125118 7.712067e-07\n```\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe that the minimum energy corresponding to the random training\nset is already close to the ground state energy\n`grd_E=-1.1372633205048763 Ha` with an error of around `2.81e-04 Ha`.\nBut we notice that the maximum energy is relatively far so the random\nsequences give a wider spread of energies.\n\nThe energies of the generated sequences from the latest and the best GPT\nmodel versions however have a narrower spread and so, the average and\nthe minimum energies are very close to `grd_E`, closer than those in the\nrandom training set. Namely, around a `2.25e-04 Ha` error for the\nminimum energy generated by the latest model version and a `7.71e-07 Ha`\nerror for the best model version. It is then very interesting that a\nwell-trained GPT model can generate sequences that are better than those\nin the training set, even though every sequence the model has seen just\ncame from that set. That is, the model was able to generalize.\n\nBetween the two GPT model versions, we see that the latest version is\nworse than the best version. The minimum energy error for the latest\nversion has the same order of magnitude as the corresponding one for the\nrandom training set. Contrast this with the minimum energy error for the\nbest version, which is 3 orders of magnitude smaller. This behavior is\nsupported by our observation in Figure 4 where the performance of the\nmodel after the 7,000th iteration worsened. That is, the predicted\nenergies started to deviate further from the true energies which in turn\ncaused the states being sampled from these predicted energies to be\ndifferent from the intended lower energy states.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n\nIn this demo, we see that GPT-QE is a viable alternative in estimating\nthe ground-state energy of a hydrogen molecule. The best underlying GPT\nmodel version can generate a state whose energy is only around\n`7.71e-07 Ha` away from ground state energy, which is well below the\nchemical accuracy. Additionally, since the GPT model being optimized is\ncompletely detached from the quantum simulations, gradients across a\npotentially large quantum circuit don\\'t need to be computed, for\nexample. Thus, the machinery of classical ML can be harnessed without\nworrying too much about the quantum algorithm side of the problem.\n\nThe reader can also experiment with other molecules from PennyLane and\ntweak several hyperparameters of the GPT model (like `dropout`, and\n`n_layer`) and include standard ML callbacks to its training (like an\nearly stopping mechanism and a learning rate schedule). The code itself\nis also open to further optimization. For instance, using [PennyLane\nLightning](https://docs.pennylane.ai/projects/lightning/en/stable/index.html)\nto evaluate the energies faster. An online training loop can also be\nimplemented similarly to our offline version, the reader would just need\nto sample sequences from the current GPT model instead of a fixed\ndataset for each training iteration. To facilitate exploration and\nexploitation, one would also define a schedule for the inverse\ntemperature. That is, initially letting the GPT model sample more\nrandomly through a high temperature, then gradually decreasing so that\nthe GPT model focuses more on the higher probability states (low\nenergies).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# About the authors\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}